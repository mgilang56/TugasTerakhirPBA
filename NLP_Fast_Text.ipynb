{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Dataset Class\n",
        "class YahooAnswersDataset(Dataset):\n",
        "    def __init__(self, data, labels, word_embeddings):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "        self.word_embeddings = word_embeddings\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.data[idx]\n",
        "        label = self.labels[idx]\n",
        "        vectorized_text = self.vectorize_text(text)\n",
        "        return torch.tensor(vectorized_text, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "    def vectorize_text(self, text):\n",
        "        if not isinstance(text, str):  # Periksa apakah teks adalah string\n",
        "            text = \"\"  # Jika bukan, ubah menjadi string kosong\n",
        "        tokens = text.split()\n",
        "        vectors = [self.word_embeddings[word] for word in tokens if word in self.word_embeddings]\n",
        "        if not vectors:\n",
        "            return np.zeros(100)  # Embedding size = 100\n",
        "        return np.mean(vectors, axis=0)\n",
        "\n",
        "\n",
        "# Model Class\n",
        "class FastTextModel(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_classes):\n",
        "        super(FastTextModel, self).__init__()\n",
        "        self.fc = nn.Linear(embedding_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "# Load Data\n",
        "# Menambahkan nama kolom secara manual\n",
        "column_names = ['label', 'question_title', 'question_content', 'best_answer']\n",
        "\n",
        "# Membaca file CSV\n",
        "train_data = pd.read_csv('/content/train.csv', names=column_names)\n",
        "test_data = pd.read_csv('/content/test.csv', names=column_names)\n",
        "\n",
        "# Gabungkan kolom teks menjadi satu\n",
        "train_data['text'] = train_data['question_title'] + \" \" + train_data['question_content']\n",
        "test_data['text'] = test_data['question_title'] + \" \" + test_data['question_content']\n",
        "\n",
        "# Mengatur ulang label agar dimulai dari 0\n",
        "train_data['label'] = train_data['label'] - 1\n",
        "test_data['label'] = test_data['label'] - 1\n",
        "\n",
        "# Validasi jumlah kelas\n",
        "num_classes = train_data['label'].nunique()\n",
        "print(f\"Jumlah kelas unik: {num_classes}\")\n",
        "\n",
        "\n",
        "# Load Word Embeddings\n",
        "embedding_path = '/content/glove.6B.100d.txt'\n",
        "embeddings_index = {}\n",
        "with open(embedding_path, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "# Prepare Dataset\n",
        "X_train, X_val, y_train, y_val = train_test_split(train_data['text'], train_data['label'], test_size=0.2, random_state=42)\n",
        "train_dataset = YahooAnswersDataset(X_train.tolist(), y_train.tolist(), embeddings_index)\n",
        "val_dataset = YahooAnswersDataset(X_val.tolist(), y_val.tolist(), embeddings_index)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "\n",
        "# Model, Loss, Optimizer\n",
        "model = FastTextModel(embedding_dim=100, num_classes=train_data['label'].nunique())\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training Loop\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=5):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}, Val Accuracy: {correct/total:.4f}\")\n",
        "\n",
        "train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=3)\n",
        "\n",
        "# Evaluation on Test Data\n",
        "def evaluate_model(model, test_data, word_embeddings):\n",
        "    test_data['text'] = test_data['question_title'] + \" \" + test_data['question_content']\n",
        "    test_dataset = YahooAnswersDataset(test_data['text'].tolist(), test_data['label'].tolist(), word_embeddings)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f\"Test Accuracy: {correct / total:.4f}\")\n",
        "\n",
        "evaluate_model(model, test_data, embeddings_index)\n",
        "\n",
        "# Save the Model\n",
        "def save_model(model, path):\n",
        "    torch.save(model.state_dict(), path)\n",
        "    print(f\"Model saved to {path}\")\n",
        "\n",
        "save_model(model, '/content/fasttext_model.pth')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LbxlTBSwpAkO",
        "outputId": "2c318f86-6605-453f-eafc-2b663d53361a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jumlah kelas unik: 10\n",
            "Epoch 1/3, Train Loss: 1.8415, Val Loss: 1.7947, Val Accuracy: 0.3713\n",
            "Epoch 2/3, Train Loss: 1.7894, Val Loss: 1.7863, Val Accuracy: 0.3735\n",
            "Epoch 3/3, Train Loss: 1.7842, Val Loss: 1.7833, Val Accuracy: 0.3754\n",
            "Test Accuracy: 0.3775\n",
            "Model saved to /content/fasttext_model.pth\n"
          ]
        }
      ]
    }
  ]
}